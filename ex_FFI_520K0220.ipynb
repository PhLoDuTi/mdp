{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZwj62F/wZdYLGApumGnOR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Remember to upload `baskets-df.csv` before running.***\n",
        "\n"
      ],
      "metadata": {
        "id": "w-5uZNGFhT-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing PySpark"
      ],
      "metadata": {
        "id": "571qvlXpe2ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWf-A61oeujL",
        "outputId": "8c4cf05f-b021-47b5-991f-419460f95472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-08 08:52:30--  http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228721937 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.1-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.1-bin-had 100%[===================>] 218.13M  1.81MB/s    in 1m 43s  \n",
            "\n",
            "2023-03-08 08:54:13 (2.12 MB/s) - ‘spark-3.1.1-bin-hadoop3.2.tgz’ saved [228721937/228721937]\n",
            "\n",
            "cp: missing destination file operand after 'drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz'\n",
            "Try 'cp --help' for more information.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!cp drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "W-HXDZ91fPj4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTezebnfRIB",
        "outputId": "dfe53fc4-c2d7-41be-fca3-d7da4c98e21d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.1.1-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "dEMIPPCLfS2P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init"
      ],
      "metadata": {
        "id": "txIft0-Nksxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Create a SparkConf object and a SparkContext\n",
        "conf = SparkConf().setAppName(\"CovidDataProcessing\")\n",
        "sc = SparkContext.getOrCreate(conf)"
      ],
      "metadata": {
        "id": "0cf5nzwxkq-p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "6adC3_szko4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# define schema for the CSV file\n",
        "schema = StructType([\n",
        "    StructField(\"BillNo\", StringType(), True),\n",
        "    StructField(\"Itemname\", StringType(), True),\n",
        "    StructField(\"Quantity\", IntegerType(), True),\n",
        "    StructField(\"Date\", StringType(), True),\n",
        "    StructField(\"Price\", DoubleType(), True),\n",
        "    StructField(\"CustomerID\", StringType(), True),\n",
        "    StructField(\"Country\", StringType(), True)\n",
        "])\n",
        "\n",
        "# read the CSV file into a DataFrame with the specified schema\n",
        "df = spark.read.csv(\"baskets01.csv\", header=False, schema=schema)\n",
        "\n",
        "# group by BillNo and collect a list of Itemname\n",
        "baskets_df = df.groupBy(\"BillNo\").agg(collect_list(\"Itemname\").alias(\"basket\"))\n",
        "\n",
        "# convert the array column to a string column\n",
        "baskets_df = baskets_df.withColumn(\"basket\", concat_ws(\",\", \"basket\"))\n",
        "\n",
        "# save the DataFrame to the local filesystem as baskets-df.csv\n",
        "baskets_df.write.csv(\"baskets-df.csv\", header=True, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "j9pbPnCgiWCG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "mA5_8WGuom3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "from pyspark.sql.functions import array_distinct\n",
        "\n",
        "# remove duplicate rows\n",
        "df = df.dropDuplicates()\n",
        "\n",
        "# group items in the same basket together\n",
        "df = df.groupBy(\"BillNo\").agg(collect_list(\"basket\").alias(\"basket\"))\n",
        "\n",
        "# remove duplicate items in each basket\n",
        "df = df.withColumn(\"basket\", array_distinct(\"basket\"))\n",
        "\n",
        "# create FPGrowth model\n",
        "fp_growth = FPGrowth(itemsCol=\"basket\", minSupport=0.01, minConfidence=0.3)\n",
        "\n",
        "# fit the model to the dataset\n",
        "model = fp_growth.fit(df)\n",
        "\n",
        "# find frequent itemsets\n",
        "freq_itemsets = model.freqItemsets\n",
        "\n",
        "# find association rules\n",
        "assoc_rules = model.associationRules"
      ],
      "metadata": {
        "id": "_pj1OceMlc1T"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert freq_itemsets and assoc_rules to string columns\n",
        "from pyspark.sql.functions import to_json, regexp_replace\n",
        "\n",
        "freq_itemsets_str = freq_itemsets.withColumn(\"items\", regexp_replace(to_json(\"items\"), \"\\[|\\]\", \"\"))\n",
        "assoc_rules_str = assoc_rules.withColumn(\"antecedent\", regexp_replace(to_json(\"antecedent\"), \"\\[|\\]\", \"\")) \\\n",
        "                             .withColumn(\"consequent\", regexp_replace(to_json(\"consequent\"), \"\\[|\\]\", \"\"))\n"
      ],
      "metadata": {
        "id": "Ha2t8NWOnrQb"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_itemsets_str.write.csv(\"freq_itemsets_str.csv\", header=True, mode=\"overwrite\")\n",
        "assoc_rules_str.write.csv(\"assoc_rules_str.csv\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "0kvUcGXXn6o8"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}
