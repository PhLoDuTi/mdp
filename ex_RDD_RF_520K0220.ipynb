{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqG5HN5M3aANYs3ZAu5PR/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Remember to upload the COVID-19 dataset (`WHO-COVID-19-global.csv`) before running.***"
      ],
      "metadata": {
        "id": "w-5uZNGFhT-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing PySpark"
      ],
      "metadata": {
        "id": "571qvlXpe2ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWf-A61oeujL",
        "outputId": "312a260b-05c0-49b7-affb-1205ba3ccac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!cp drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "W-HXDZ91fPj4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTezebnfRIB",
        "outputId": "3cae3c95-8f0a-4c0c-b8f1-78df0f1a257a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.1.1-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "dEMIPPCLfS2P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init"
      ],
      "metadata": {
        "id": "txIft0-Nksxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Create a SparkConf object and a SparkContext\n",
        "conf = SparkConf().setAppName(\"CovidDataProcessing\")\n",
        "sc = SparkContext.getOrCreate(conf)"
      ],
      "metadata": {
        "id": "0cf5nzwxkq-p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "6adC3_szko4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the CSV file into an RDD\n",
        "    rdd = sc.textFile(\"WHO-COVID-19-global.csv\")\n",
        "\n",
        "    # Split the header row by commas to count the number of columns\n",
        "    header_cols = next(csv.reader([rdd.first()]))\n",
        "    num_cols = len(header_cols)\n",
        "\n",
        "    # Remove the header row from the RDD\n",
        "    rdd = rdd.filter(lambda row: row != header_cols)\n",
        "\n",
        "    # Parse each row using the csv module and filter out rows with unexpected numbers of columns\n",
        "    parsed_rdd = rdd.map(lambda row: next(csv.reader([row])))\n",
        "    parsed_rdd = parsed_rdd.filter(lambda row: len(row) == num_cols)\n",
        "\n",
        "    # Define the column indices for the numeric columns\n",
        "    numeric_cols = [i for i, col in enumerate(header_cols) if col.startswith(\"Cases\") or col.startswith(\"Deaths\")]\n",
        "\n",
        "    # Group the RDD by the \"WHO Region\" column\n",
        "    grouped_rdd = parsed_rdd.groupBy(lambda row: row[1])\n",
        "\n",
        "    # Compute the sum of each numeric column for each group\n",
        "    totals = {}\n",
        "    for col_idx in numeric_cols:\n",
        "        col_name = header_cols[col_idx]\n",
        "        total_col = grouped_rdd.mapValues(lambda rows: round(sum(float(row[col_idx].replace(',', '')) if row[col_idx].replace(',', '').replace('.', '').isnumeric() else 0 for row in rows), 2))\n",
        "        totals[col_name] = total_col.collect()\n",
        "\n",
        "    # Display the results\n",
        "    for col_name, result in totals.items():\n",
        "        print(f\"Total {col_name}:\")\n",
        "        for region, total in result:\n",
        "            print(f\"  {region}: {total}\")\n",
        "\n",
        "except csv.Error as e:\n",
        "    print(f\"Error parsing CSV file: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e}\")\n",
        "\n",
        "finally:\n",
        "    # Stop the SparkContext\n",
        "    sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5zc9EfOk2H9",
        "outputId": "14af3a02-c097-4b35-8bd9-048caf571649"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Cases - cumulative total:\n",
            "  Americas: 176342137.0\n",
            "  South-East Asia: 60084208.0\n",
            "  Western Pacific: 85868508.0\n",
            "  WHO Region: 0\n",
            "  Europe: 249105808.0\n",
            "  Eastern Mediterranean: 23011442.0\n",
            "  Africa: 9298893.0\n",
            "  Other: 764.0\n",
            "Total Cases - cumulative total per 100000 population:\n",
            "  Americas: 1208742.01\n",
            "  South-East Asia: 65053.02\n",
            "  Western Pacific: 607137.07\n",
            "  WHO Region: 0\n",
            "  Europe: 2038171.58\n",
            "  Eastern Mediterranean: 178510.43\n",
            "  Africa: 203666.51\n",
            "  Other: 0.0\n",
            "Total Cases - newly reported in last 7 days:\n",
            "  Americas: 696441.0\n",
            "  South-East Asia: 82449.0\n",
            "  Western Pacific: 1806067.0\n",
            "  WHO Region: 0\n",
            "  Europe: 988960.0\n",
            "  Eastern Mediterranean: 28094.0\n",
            "  Africa: 10542.0\n",
            "  Other: 0.0\n",
            "Total Cases - newly reported in last 7 days per 100000 population:\n",
            "  Americas: 4434.75\n",
            "  South-East Asia: 61.92\n",
            "  Western Pacific: 5451.1\n",
            "  WHO Region: 0\n",
            "  Europe: 5609.7\n",
            "  Eastern Mediterranean: 360.17\n",
            "  Africa: 935.76\n",
            "  Other: 0.0\n",
            "Total Cases - newly reported in last 24 hours:\n",
            "  Americas: 29643.0\n",
            "  South-East Asia: 10916.0\n",
            "  Western Pacific: 247343.0\n",
            "  WHO Region: 0\n",
            "  Europe: 182316.0\n",
            "  Eastern Mediterranean: 2933.0\n",
            "  Africa: 238.0\n",
            "  Other: 0.0\n",
            "Total Deaths - cumulative total:\n",
            "  Americas: 2821706.0\n",
            "  South-East Asia: 796201.0\n",
            "  Western Pacific: 263345.0\n",
            "  WHO Region: 0\n",
            "  Europe: 2080678.0\n",
            "  Eastern Mediterranean: 347843.0\n",
            "  Africa: 174350.0\n",
            "  Other: 13.0\n",
            "Total Deaths - cumulative total per 100000 population:\n",
            "  Americas: 9417.27\n",
            "  South-East Asia: 385.43\n",
            "  Western Pacific: 1600.88\n",
            "  WHO Region: 0\n",
            "  Europe: 13557.15\n",
            "  Eastern Mediterranean: 1461.85\n",
            "  Africa: 1435.91\n",
            "  Other: 0.0\n",
            "Total Deaths - newly reported in last 7 days:\n",
            "  Americas: 4500.0\n",
            "  South-East Asia: 531.0\n",
            "  Western Pacific: 3555.0\n",
            "  WHO Region: 0\n",
            "  Europe: 2626.0\n",
            "  Eastern Mediterranean: 299.0\n",
            "  Africa: 42.0\n",
            "  Other: 0.0\n",
            "Total Deaths - newly reported in last 7 days per 100000 population:\n",
            "  Americas: 13.14\n",
            "  South-East Asia: 0.42\n",
            "  Western Pacific: 6.97\n",
            "  WHO Region: 0\n",
            "  Europe: 24.58\n",
            "  Eastern Mediterranean: 0.7\n",
            "  Africa: 1.53\n",
            "  Other: 0.0\n",
            "Total Deaths - newly reported in last 24 hours:\n",
            "  Americas: 137.0\n",
            "  South-East Asia: 70.0\n",
            "  Western Pacific: 514.0\n",
            "  WHO Region: 0\n",
            "  Europe: 523.0\n",
            "  Eastern Mediterranean: 33.0\n",
            "  Africa: 1.0\n",
            "  Other: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "mA5_8WGuom3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"WHO-COVID19\").getOrCreate()\n",
        "\n",
        "# read CSV data into a DataFrame\n",
        "df = spark.read.csv(\"WHO-COVID-19-global.csv\", header=True)\n",
        "\n",
        "# select columns of interest and cast them to appropriate types\n",
        "df = df.selectExpr(\"CAST(`Cases - cumulative total` AS INT) AS cases_total\",\n",
        "                   \"CAST(`Cases - cumulative total per 100000 population` AS FLOAT) AS cases_total_per_100k\",\n",
        "                   \"CAST(`Cases - newly reported in last 7 days` AS INT) AS cases_last_7_days\",\n",
        "                   \"CAST(`Cases - newly reported in last 7 days per 100000 population` AS FLOAT) AS cases_last_7_days_per_100k\",\n",
        "                   \"CAST(`Cases - newly reported in last 24 hours` AS INT) AS cases_last_24_hours\",\n",
        "                   \"CAST(`Deaths - cumulative total` AS INT) AS deaths_total\",\n",
        "                   \"CAST(`Deaths - cumulative total per 100000 population` AS FLOAT) AS deaths_total_per_100k\",\n",
        "                   \"CAST(`Deaths - newly reported in last 7 days` AS INT) AS deaths_last_7_days\",\n",
        "                   \"CAST(`Deaths - newly reported in last 7 days per 100000 population` AS FLOAT) AS deaths_last_7_days_per_100k\",\n",
        "                   \"CAST(`Deaths - newly reported in last 24 hours` AS INT) AS deaths_last_24_hours\",\n",
        "                   \"`WHO Region`\")\n",
        "\n",
        "# group by WHO Region and compute the sum of each column\n",
        "grouped_df = df.groupBy(\"WHO Region\").sum()\n",
        "\n",
        "# write the result to disk\n",
        "grouped_df.write.mode(\"overwrite\").csv(\"WHO-COVID19-global/\")\n"
      ],
      "metadata": {
        "id": "bffrI1QHon9u"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}